{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# import contractions\n",
    "# import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_links_characters(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    return text\n",
    "\n",
    "def regular_preprocess(text):\n",
    "    text = remove_html(text)\n",
    "    text = remove_links_characters(text)\n",
    "    # text = replace_contractions(text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(docs, stopwords):\n",
    "    docs_ref = []\n",
    "    for doc in docs:\n",
    "        word_list = doc.lower().split()\n",
    "        word_list_ref = [word for word in word_list if word not in stopwords]\n",
    "        word_str_ref = ' '.join(word_list_ref)\n",
    "        docs_ref.append(word_str_ref)\n",
    "    return docs_ref\n",
    "\n",
    "def stem_words(docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    for doc in docs:\n",
    "        word_list = doc.lower().split()\n",
    "        for word in word_list:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        stems_str = ' '.join(stems)\n",
    "        stems.append(stems_str)\n",
    "    return stems\n",
    "\n",
    "def preprocess(data):\n",
    "    refined_data = []\n",
    "    for dp in data:\n",
    "        refined_data.append(regular_preprocess(dp))        \n",
    "    return refined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copy contents of all files in both folders into a list\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# copy contents of all files in both folders into a list\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# train data\n",
    "train_neg = glob.glob(os.path.join(os.getcwd(), \"Dataset/train/neg\", \"*.txt\"))\n",
    "for f_path in train_neg:\n",
    "    with open(f_path) as f:\n",
    "        train_data.append(f.read())\n",
    "\n",
    "train_pos = glob.glob(os.path.join(os.getcwd(), \"Dataset/train/pos\", \"*.txt\"))\n",
    "for f_path in train_pos:\n",
    "    with open(f_path) as f:\n",
    "        train_data.append(f.read())\n",
    "\n",
    "# test data\n",
    "def sort_nicely(l):\n",
    "# Sort the given list in the way that humans expect.\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
    "    l.sort(key=alphanum_key)\n",
    "\n",
    "test_files = glob.glob(os.path.join(os.getcwd(), \"Dataset/test\", \"*.txt\"))\n",
    "sort_nicely(test_files)\n",
    "test_files_ids = [int(re.sub(\"[^0-9]\",\"\", item)) for item in test_files]\n",
    "\n",
    "for f_path in test_files:\n",
    "    with open(f_path) as f:\n",
    "        test_data.append(f.read())\n",
    "\n",
    "# targets: first 12500 are pos, next 12500 are neg\n",
    "targets = [0 if i<12500 else 1 for i in range(25000)]\n",
    "\n",
    "# with open('english') as f:\n",
    "#     stopwords = f.read().splitlines()\n",
    "    \n",
    "# print(train_data[0])\n",
    "train_data_clean = preprocess(train_data)\n",
    "# train_data_clean = remove_stopwords(train_data_clean, stopwords)\n",
    "# train_data_clean = stem_words(train_data_clean)\n",
    "# with open('train_stemmed.txt', 'w') as f:\n",
    "#     for item in train_data_clean:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# print(train_data_clean[0])\n",
    "\n",
    "test_data_clean = preprocess(test_data)\n",
    "# test_data_clean = remove_stopwords(test_data_clean, stopwords)\n",
    "# test_data_clean = stem_words(test_data_clean)\n",
    "\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train_data_clean, targets, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "pclf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features=None, ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer(norm='l2')),\n",
    "    ('norm', Normalizer(norm='l2')),\n",
    "    ('clf', SGDClassifier(alpha=1e-05)),\n",
    "])\n",
    "            \n",
    "pclf.fit(X_train, y_train)\n",
    "y_pred_1 = pclf.predict(test_data_clean)\n",
    "\n",
    "# grid_search = GridSearchCV(pclf, params, cv=5, n_jobs=-1, verbose=1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# y1_pred = grid_search.predict(X_validation)\n",
    "# y_pred = grid_search.predict(test_data_clean)\n",
    "\n",
    "# def display_results(y_val, y_pred):\n",
    "#     print(metrics.classification_report(y_val, y_pred))\n",
    "#     print(\"Accuracy % = \", metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "with open('submission.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"Id\", \"Category\"))\n",
    "    writer.writerows(zip(test_files_ids, y_pred_1))\n",
    "\n",
    "# display_results(y_validation, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('norm', Normalizer(copy=True, norm='l2')), ('clf', SGDClassifier(alpha=1e-05, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))], 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'norm': Normalizer(copy=True, norm='l2'), 'clf': SGDClassifier(alpha=1e-05, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'norm__copy': True, 'norm__norm': 'l2', 'clf__alpha': 1e-05, 'clf__average': False, 'clf__class_weight': None, 'clf__early_stopping': False, 'clf__epsilon': 0.1, 'clf__eta0': 0.0, 'clf__fit_intercept': True, 'clf__l1_ratio': 0.15, 'clf__learning_rate': 'optimal', 'clf__loss': 'hinge', 'clf__max_iter': 5, 'clf__n_iter': None, 'clf__n_iter_no_change': 5, 'clf__n_jobs': None, 'clf__penalty': 'l2', 'clf__power_t': 0.5, 'clf__random_state': None, 'clf__shuffle': True, 'clf__tol': None, 'clf__validation_fraction': 0.1, 'clf__verbose': 0, 'clf__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "print(best_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy contents of all files in both folders into a list\n",
    "import glob\n",
    "import os\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import csv\n",
    "\n",
    "\n",
    "# copy contents of all files in both folders into a list\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# train data\n",
    "train_neg = glob.glob(os.path.join(os.getcwd(), \"Dataset/train/neg\", \"*.txt\"))\n",
    "for f_path in train_neg:\n",
    "    with open(f_path) as f:\n",
    "        train_data.append(f.read())\n",
    "\n",
    "train_pos = glob.glob(os.path.join(os.getcwd(), \"Dataset/train/pos\", \"*.txt\"))\n",
    "for f_path in train_pos:\n",
    "    with open(f_path) as f:\n",
    "        train_data.append(f.read())\n",
    "# print(train_data[0])\n",
    "# print(preprocess(train_data[0]))\n",
    "\n",
    "# test data\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "test_files = (glob.glob(os.path.join(os.getcwd(), \"Dataset/test\", \"*.txt\")))\n",
    "for f_path in test_files:\n",
    "    with open(f_path) as f:\n",
    "        test_data.append(f.read())\n",
    "\n",
    "# targets: first 12500 are pos, next 12500 are neg\n",
    "targets = [0 if i<12500 else 1 for i in range(25000)]\n",
    "\n",
    "train_data_clean = preprocess(train_data)\n",
    "test_data_clean = preprocess(test_data)\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train_data_clean, targets, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "pclf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('norm', Normalizer()),\n",
    "    ('clf', LinearSVC(C=0.5)),\n",
    "])\n",
    "\n",
    "# params = {\n",
    "# #     'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#     'vect__ngram_range': ((1, 2),(1, 1),(2,2)),  # unigrams or bigrams\n",
    "# #     'clf__max_iter': (5,),\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pclf, params, cv=5, n_jobs=-1, verbose=1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# y1_pred = grid_search.predict(X_validation)\n",
    "# y_pred = grid_search.predict(test_data_clean)\n",
    "\n",
    "\n",
    "def display_results(y_val, y_pred):\n",
    "    print(metrics.classification_report(y_val, y_pred))\n",
    "    print(\"Accuracy % = \", metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "pclf.fit(X_train, y_train)\n",
    "y_pred_2 = pclf.predict(test_data_clean)\n",
    "# print(y_pred)\n",
    "\n",
    "with open('submissionLSVM.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"Id\", \"Category\"))\n",
    "    writer.writerows(zip(test_files_ids, y_pred_2))\n",
    "\n",
    "# display_results(y_validation, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy contents of all files in both folders into a list\n",
    "import glob\n",
    "import os\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import csv\n",
    "\n",
    "\n",
    "# copy contents of all files in both folders into a list\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# train data\n",
    "train_neg = glob.glob(os.path.join(os.getcwd(), \"Dataset/train/neg\", \"*.txt\"))\n",
    "for f_path in train_neg:\n",
    "    with open(f_path) as f:\n",
    "        train_data.append(f.read())\n",
    "\n",
    "train_pos = glob.glob(os.path.join(os.getcwd(), \"Dataset/train/pos\", \"*.txt\"))\n",
    "for f_path in train_pos:\n",
    "    with open(f_path) as f:\n",
    "        train_data.append(f.read())\n",
    "# print(train_data[0])\n",
    "# print(preprocess(train_data[0]))\n",
    "\n",
    "# test data\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "test_files = (glob.glob(os.path.join(os.getcwd(), \"Dataset/test\", \"*.txt\")))\n",
    "for f_path in test_files:\n",
    "    with open(f_path) as f:\n",
    "        test_data.append(f.read())\n",
    "\n",
    "# targets: first 12500 are pos, next 12500 are neg\n",
    "targets = [0 if i<12500 else 1 for i in range(25000)]\n",
    "\n",
    "train_data_clean = preprocess(train_data)\n",
    "test_data_clean = preprocess(test_data)\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train_data_clean, targets, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "pclf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('norm', Normalizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "def display_results(y_val, y_pred):\n",
    "    print(metrics.classification_report(y_val, y_pred))\n",
    "    print(\"Accuracy % = \", metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "pclf.fit(X_train, y_train)\n",
    "y_pred_3 = pclf.predict(test_data_clean)\n",
    "# print(y_pred)\n",
    "\n",
    "with open('submissionLR.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"Id\", \"Category\"))\n",
    "    writer.writerows(zip(test_files_ids, y_pred_3))\n",
    "\n",
    "# display_results(y_validation, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SubmissionEnsemble.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow((\"SGD\", \"SVM\",\"LR\"))\n",
    "    writer.writerows(zip(y_pred_1,y_pred_2,y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "preds = pd.DataFrame()\n",
    "preds['pred1'] = y_pred_1\n",
    "preds['pred2'] = y_pred_2\n",
    "preds['pred3'] = y_pred_3\n",
    "\n",
    "preds['maj_vote'] = preds.mode(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>maj_vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred1  pred2  pred3  maj_vote\n",
       "0          0      1      1         1\n",
       "1          0      0      0         0\n",
       "2          0      1      1         1\n",
       "3          0      0      0         0\n",
       "4          1      1      1         1\n",
       "5          0      0      0         0\n",
       "6          0      0      0         0\n",
       "7          0      1      1         1\n",
       "8          0      1      1         1\n",
       "9          0      0      1         0\n",
       "10         0      0      0         0\n",
       "11         0      1      1         1\n",
       "12         1      0      0         0\n",
       "13         0      1      1         1\n",
       "14         1      0      0         0\n",
       "15         0      0      0         0\n",
       "16         0      1      1         1\n",
       "17         1      0      0         0\n",
       "18         0      0      0         0\n",
       "19         0      1      1         1\n",
       "20         0      0      0         0\n",
       "21         0      0      0         0\n",
       "22         0      0      0         0\n",
       "23         1      0      0         0\n",
       "24         1      1      1         1\n",
       "25         1      0      0         0\n",
       "26         0      1      1         1\n",
       "27         0      1      1         1\n",
       "28         0      0      0         0\n",
       "29         1      0      0         0\n",
       "...      ...    ...    ...       ...\n",
       "24970      0      0      0         0\n",
       "24971      0      1      1         1\n",
       "24972      1      1      1         1\n",
       "24973      0      0      0         0\n",
       "24974      0      0      0         0\n",
       "24975      1      1      1         1\n",
       "24976      0      1      1         1\n",
       "24977      1      1      0         1\n",
       "24978      0      0      0         0\n",
       "24979      1      0      0         0\n",
       "24980      1      1      0         1\n",
       "24981      0      0      1         0\n",
       "24982      0      0      0         0\n",
       "24983      1      1      1         1\n",
       "24984      1      1      1         1\n",
       "24985      0      1      1         1\n",
       "24986      1      0      0         0\n",
       "24987      0      1      0         0\n",
       "24988      0      1      1         1\n",
       "24989      0      1      1         1\n",
       "24990      1      0      0         0\n",
       "24991      1      0      1         1\n",
       "24992      1      1      1         1\n",
       "24993      0      0      0         0\n",
       "24994      1      0      0         0\n",
       "24995      1      0      0         0\n",
       "24996      1      0      0         0\n",
       "24997      1      0      0         0\n",
       "24998      1      1      1         1\n",
       "24999      1      0      0         0\n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['true_val'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def advanced_preprocess(text):\n",
    "# #     words = nltk.word_tokenize(text)\n",
    "#     words = replace_numbers(text)\n",
    "#     words = remove_non_ascii(words)\n",
    "#     words = to_lowercase(words)\n",
    "#     words = remove_stopwords(words)\n",
    "#     words = stem_words(words)\n",
    "#     return words\n",
    "\n",
    "# def replace_contractions(text):\n",
    "#     \"\"\"Replace contractions in string of text\"\"\"\n",
    "#     return contractions.fix(text)\n",
    "#\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     stemmer = PorterStemmer()\n",
    "#     stems = []\n",
    "#     for word in words:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stems.append(stem)\n",
    "#     return stems\n",
    "#\n",
    "# def replace_numbers(words):\n",
    "#     \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "#     p = inflect.engine()\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         if word.isdigit():\n",
    "#             new_word = p.number_to_words(word)\n",
    "#             new_words.append(new_word)\n",
    "#         else:\n",
    "#             new_words.append(word)\n",
    "#     return new_words\n",
    "#\n",
    "# def lemmatize_verbs(words):\n",
    "#     \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmas = []\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#         lemmas.append(lemma)\n",
    "#     return lemmas\n",
    "#\n",
    "# def to_lowercase(words):\n",
    "#     \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = word.lower()\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "#\n",
    "# def remove_non_ascii(words):\n",
    "#     \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bag of Words vectorization\n",
    "cv = CountVectorizer(binary=True).fit(X_train)\n",
    "X_train_counts = cv.transform(X_train)\n",
    "X_validation_counts = cv.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      2488\n",
      "           1       0.84      0.88      0.86      2512\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      5000\n",
      "   macro avg       0.86      0.86      0.86      5000\n",
      "weighted avg       0.86      0.86      0.86      5000\n",
      "\n",
      "Accuracy % =  0.8556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_NB = MultinomialNB().fit(X_train_counts, y_train)\n",
    "y_pred = clf_NB.predict(X_validation_counts)\n",
    "\n",
    "# X_train_normalized = np.array(X_train_normalized)\n",
    "\n",
    "# print(X_train_normalized.shape)\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      2488\n",
      "           1       0.89      0.87      0.88      2512\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n",
      "Accuracy % =  0.8814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_LR = LogisticRegression().fit(X_train_counts, y_train)\n",
    "y_pred = clf_LR.predict(X_validation_counts)\n",
    "\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# clf_DT = DecisionTreeClassifier().fit(X_train_counts, y_train)\n",
    "# y_pred = clf_DT.predict(X_validation_counts)\n",
    "\n",
    "# print(metrics.classification_report(y_validation, y_pred))\n",
    "# print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87      2488\n",
      "           1       0.87      0.86      0.87      2512\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n",
      "Accuracy % =  0.8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_SVM = LinearSVC().fit(X_train_counts, y_train)\n",
    "y_pred = clf_SVM.predict(X_validation_counts)\n",
    "\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer(binary=True) with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Bag of Words vectorization\n",
    "cv = CountVectorizer(binary=True).fit(X_train)\n",
    "X_train_counts = cv.transform(X_train)\n",
    "X_validation_counts = cv.transform(X_validation)\n",
    "\n",
    "# tfidf\n",
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
    "X_validation_tfidf = tfidf_transformer.transform(X_validation_counts)\n",
    "\n",
    "# normalization\n",
    "normalizer_tranformer = Normalizer().fit(X=X_train_tfidf)\n",
    "X_train_normalized = normalizer_tranformer.transform(X_train_tfidf)\n",
    "X_validation_normalized = normalizer_tranformer.transform(X_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68499)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      2488\n",
      "           1       0.85      0.89      0.87      2512\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n",
      "Accuracy % =  0.8654\n"
     ]
    }
   ],
   "source": [
    "clf_NB = MultinomialNB().fit(X_train_normalized, y_train)\n",
    "y_pred = clf_NB.predict(X_validation_normalized)\n",
    "\n",
    "# X_train_normalized = np.array(X_train_normalized)\n",
    "\n",
    "print(X_train_normalized.shape)\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      2488\n",
      "           1       0.91      0.88      0.89      2512\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "Accuracy % =  0.8944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_LR = LogisticRegression().fit(X_train_normalized, y_train)\n",
    "y_pred = clf_LR.predict(X_validation_normalized)\n",
    "\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# clf_DT = DecisionTreeClassifier().fit(X_train_normalized, y_train)\n",
    "# y_pred = clf_DT.predict(X_validation_normalized)\n",
    "\n",
    "# print(metrics.classification_report(y_validation, y_pred))\n",
    "# print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      2488\n",
      "           1       0.90      0.88      0.89      2512\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "Accuracy % =  0.8946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_SVM = LinearSVC().fit(X_train_normalized, y_train)\n",
    "y_pred = clf_SVM.predict(X_validation_normalized)\n",
    "\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words vectorization\n",
    "cv = CountVectorizer().fit(X_train)\n",
    "X_train_counts = cv.transform(X_train)\n",
    "X_validation_counts = cv.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84      2488\n",
      "           1       0.83      0.88      0.85      2512\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n",
      "Accuracy % =  0.846\n"
     ]
    }
   ],
   "source": [
    "clf_NB = MultinomialNB().fit(X_train_counts, y_train)\n",
    "y_pred = clf_NB.predict(X_validation_counts)\n",
    "\n",
    "# X_train_normalized = np.array(X_train_normalized)\n",
    "\n",
    "# print(X_train_normalized.shape)\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      2488\n",
      "           1       0.90      0.88      0.89      2512\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "Accuracy % =  0.8896\n"
     ]
    }
   ],
   "source": [
    "clf_LR = LogisticRegression().fit(X_train_counts, y_train)\n",
    "y_pred = clf_LR.predict(X_validation_counts)\n",
    "\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_DT = DecisionTreeClassifier().fit(X_train_counts, y_train)\n",
    "# y_pred = clf_DT.predict(X_validation_counts)\n",
    "\n",
    "# print(metrics.classification_report(y_validation, y_pred))\n",
    "# print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      2488\n",
      "           1       0.88      0.86      0.87      2512\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n",
      "Accuracy % =  0.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_SVM = LinearSVC().fit(X_train_counts, y_train)\n",
    "y_pred = clf_SVM.predict(X_validation_counts)\n",
    "\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer() with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bag of Words vectorization\n",
    "cv = CountVectorizer().fit(X_train)\n",
    "X_train_counts = cv.transform(X_train)\n",
    "X_validation_counts = cv.transform(X_validation)\n",
    "\n",
    "# tfidf\n",
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
    "X_validation_tfidf = tfidf_transformer.transform(X_validation_counts)\n",
    "\n",
    "# normalization\n",
    "normalizer_tranformer = Normalizer().fit(X=X_train_tfidf)\n",
    "X_train_normalized = normalizer_tranformer.transform(X_train_tfidf)\n",
    "X_validation_normalized = normalizer_tranformer.transform(X_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68354)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87      2517\n",
      "           1       0.88      0.84      0.86      2483\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n",
      "Accuracy % =  0.866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_NB = MultinomialNB().fit(X_train_normalized, y_train)\n",
    "y_pred = clf_NB.predict(X_validation_normalized)\n",
    "\n",
    "# X_train_normalized = np.array(X_train_normalized)\n",
    "\n",
    "print(X_train_normalized.shape)\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      2517\n",
      "           1       0.88      0.90      0.89      2483\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "Accuracy % =  0.8944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_LR = LogisticRegression().fit(X_train_normalized, y_train)\n",
    "y_pred = clf_LR.predict(X_validation_normalized)\n",
    "\n",
    "print(metrics.classification_report(y_validation, y_pred))\n",
    "print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_DT = DecisionTreeClassifier().fit(X_train_normalized, y_train)\n",
    "# y_pred = clf_DT.predict(X_validation_normalized)\n",
    "\n",
    "# print(metrics.classification_report(y_validation, y_pred))\n",
    "# print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.001: 0.7858\n",
      "Accuracy for C=0.005: 0.8366\n",
      "Accuracy for C=0.01: 0.857\n",
      "Accuracy for C=0.05: 0.8856\n",
      "Accuracy for C=0.1: 0.8976\n",
      "Accuracy for C=0.5: 0.899\n",
      "Accuracy for C=0.6: 0.8974\n",
      "Accuracy for C=0.55: 0.8986\n",
      "Accuracy for C=0.45: 0.899\n",
      "Accuracy for C=1.0: 0.8952\n",
      "Accuracy for C=1.5: 0.8932\n",
      "Accuracy for C=0.525: 0.8988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "for c in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.6, 0.55, 0.45, 1.0, 1.5, 0.525]:\n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train_normalized, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, metrics.accuracy_score(y_validation, svm.predict(X_validation_normalized))))\n",
    "\n",
    "# clf_SVM = LinearSVC().fit(X_train_normalized, y_train)\n",
    "# y_pred = clf_SVM.predict(X_validation_normalized)\n",
    "\n",
    "# print(metrics.classification_report(y_validation, y_pred))\n",
    "# print(\"Accuracy % = \", metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
